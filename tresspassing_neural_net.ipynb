{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import six.moves.urllib as urllib\n",
    "import sys\n",
    "import tarfile\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "import cv2\n",
    "from collections import defaultdict\n",
    "from io import StringIO\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "from object_detection.utils import label_map_util\n",
    "# This is needed since the notebook is stored in the object_detection folder.\n",
    "sys.path.append(\"..\")\n",
    "from object_detection.utils import ops as utils_ops\n",
    "import time\n",
    "if tf.__version__ < '1.4.0':\n",
    "    raise ImportError('Please upgrade your tensorflow installation to v1.4.* or later!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_NAME = 'faster_rcnn_resnet101_coco_2018_01_28'\n",
    "MODEL_NAME = 'inception'\n",
    "PATH_TO_FROZEN_GRAPH = MODEL_NAME + '/frozen_inference_graph.pb'\n",
    "PATH_TO_LABELS = 'mscoco_label_map.pbtxt'\n",
    "NUM_CLASSES = 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_graph = tf.Graph()\n",
    "with detection_graph.as_default():\n",
    "    od_graph_def = tf.GraphDef()\n",
    "    with tf.gfile.GFile(PATH_TO_FROZEN_GRAPH, 'rb') as fid:\n",
    "        serialized_graph = fid.read()\n",
    "        od_graph_def.ParseFromString(serialized_graph)\n",
    "        tf.import_graph_def(od_graph_def, name='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n",
    "categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\n",
    "category_index = label_map_util.create_category_index(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference_for_single_image(image, graph, cls):\n",
    "    with graph.as_default():\n",
    "        with tf.Session() as sess:\n",
    "            # Get handles to input and output tensors\n",
    "            ops = tf.get_default_graph().get_operations()\n",
    "            all_tensor_names = {output.name for op in ops for output in op.outputs}\n",
    "            tensor_dict = {}\n",
    "\n",
    "            for key in ['num_detections', 'detection_boxes', 'detection_scores','detection_classes', 'detection_masks']:\n",
    "                tensor_name = key + ':0'\n",
    "                if tensor_name in all_tensor_names:\n",
    "                    tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(\n",
    "                      tensor_name)\n",
    "            if 'detection_masks' in tensor_dict:\n",
    "              # The following processing is only for single image\n",
    "                detection_boxes = tf.squeeze(tensor_dict['detection_boxes'], [0])\n",
    "                detection_masks = tf.squeeze(tensor_dict['detection_masks'], [0])\n",
    "                # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.\n",
    "                real_num_detection = tf.cast(tensor_dict['num_detections'][0], tf.int32)\n",
    "                detection_boxes = tf.slice(detection_boxes, [0, 0], [real_num_detection, -1])\n",
    "                detection_masks = tf.slice(detection_masks, [0, 0, 0], [real_num_detection, -1, -1])\n",
    "                detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
    "                  detection_masks, detection_boxes, image.shape[0], image.shape[1])\n",
    "                detection_masks_reframed = tf.cast(\n",
    "                  tf.greater(detection_masks_reframed, 0.5), tf.uint8)\n",
    "                # Follow the convention by adding back the batch dimension\n",
    "                tensor_dict['detection_masks'] = tf.expand_dims(\n",
    "                  detection_masks_reframed, 0)\n",
    "            image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')\n",
    "\n",
    "            # Run inference\n",
    "            output_dict = sess.run(tensor_dict,\n",
    "                                 feed_dict={image_tensor: np.expand_dims(image, 0)})\n",
    "\n",
    "            # all outputs are float32 numpy arrays, so convert types as appropriate\n",
    "\n",
    "            output_dict['num_detections'] = int(output_dict['num_detections'][0])\n",
    "            output_dict['detection_classes'] = output_dict['detection_classes'][0].astype(np.uint8)\n",
    "            output_dict['detection_boxes'] = output_dict['detection_boxes'][0]\n",
    "            output_dict['detection_scores'] = output_dict['detection_scores'][0]\n",
    "            if 'detection_masks' in output_dict:\n",
    "                output_dict['detection_masks'] = output_dict['detection_masks'][0]\n",
    "            cls_b_p = []\n",
    "            if cls == 'bag':\n",
    "\n",
    "                for i in range(0,len(output_dict['detection_classes'])):\n",
    "                        \n",
    "                    if (int(output_dict['detection_classes'][i])) in [ 27, 31, 33]:\n",
    "\n",
    "                        ymin = int(output_dict['detection_boxes'][i][0]*image.shape[0])\n",
    "                        xmin = int(output_dict['detection_boxes'][i][1]*image.shape[1])\n",
    "                        ymax = int(output_dict['detection_boxes'][i][2]*image.shape[0])\n",
    "                        xmax = int(output_dict['detection_boxes'][i][3]*image.shape[1])\n",
    "                        if (xmin == 0 and ymin == 0 and xmax == 0 and ymax == 0):\n",
    "                            continue\n",
    "                        else:\n",
    "                            cls_b_p.append((xmin, ymin, xmax, ymax))\n",
    "            elif cls == 'person':\n",
    "                \n",
    "                for i in range(0,len(output_dict['detection_classes'])):\n",
    "                       \n",
    "                    if (int(output_dict['detection_classes'][i])) ==  1:\n",
    "                        \n",
    "                        ymin = int(output_dict['detection_boxes'][i][0]*image.shape[0])\n",
    "                        xmin = int(output_dict['detection_boxes'][i][1]*image.shape[1])\n",
    "                        ymax = int(output_dict['detection_boxes'][i][2]*image.shape[0])\n",
    "                        xmax = int(output_dict['detection_boxes'][i][3]*image.shape[1])\n",
    "                        \n",
    "                        if (xmin == 0 and ymin == 0 and xmax == 0 and ymax == 0):\n",
    "                            continue\n",
    "                        else:\n",
    "                            cls_b_p.append((xmin, ymin, xmax, ymax))\n",
    "                    \n",
    "    return cls_b_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bb_intersection_over_union(boxA, boxB):\n",
    "    # determine the (x, y)-coordinates of the intersection rectangle\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[2], boxB[2])\n",
    "    yB = min(boxA[3], boxB[3])\n",
    "\n",
    "    # compute the area of intersection rectangle\n",
    "    interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n",
    "\n",
    "    # compute the area of both the prediction and ground-truth\n",
    "    # rectangles\n",
    "    boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n",
    "    boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n",
    "\n",
    "    # compute the intersection over union by taking the intersection\n",
    "    # area and dividing it by the sum of prediction + ground-truth\n",
    "    # areas - the interesection area\n",
    "    iou = interArea / float(boxAArea + boxBArea - interArea)\n",
    "\n",
    "    # return the intersection over union value\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou_rem(bags):\n",
    "    \n",
    "    list_bag = list(bags)\n",
    "\n",
    "    if len(list_bag)>1:\n",
    "        for i in range(0, len(bags)-1):\n",
    "            if bags[i] not in list_bag:\n",
    "                continue\n",
    "            else:\n",
    "                for j in range(i+1, len(bags)):\n",
    "                    iou = bb_intersection_over_union(bags[i],bags[j])\n",
    "                    if iou>0.70:                  \n",
    "                        list_bag.remove(bags[j])\n",
    "\n",
    "    return list_bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "prev_dq = dq=deque(maxlen=300)\n",
    "\n",
    "def Unattended_object_detection(image):\n",
    "    start_x = time.time()\n",
    "    global flag\n",
    "    global prev_dq\n",
    "    global cur_bags\n",
    "   \n",
    "    counter = 0\n",
    "   \n",
    "    bags = run_inference_for_single_image(image, detection_graph, cls = 'bag')\n",
    "    bags = iou_rem(bags)\n",
    "    \n",
    "    \n",
    "    for bag in bags:\n",
    "        \n",
    "        xmin = bag[0]\n",
    "        ymin = bag[1]     \n",
    "        xmax = bag[2]\n",
    "        ymax = bag[3]\n",
    "        \n",
    "        if xmin<= 80:\n",
    "            xmin = 0\n",
    "        else:\n",
    "            xmin -= 80\n",
    "\n",
    "        if ymin<= 90:\n",
    "            ymin = 0\n",
    "        else:\n",
    "            ymin -= 90\n",
    "\n",
    "        if xmax + 80 >= image.shape[1]:\n",
    "            xmax = image.shape[1]\n",
    "        else:\n",
    "            xmax += 80\n",
    "\n",
    "        if ymax + 90 >= image.shape[0]:\n",
    "            ymax = image.shape[0]\n",
    "        else:\n",
    "            ymax += 90\n",
    "       \n",
    "        \n",
    "        persons = run_inference_for_single_image( image[ymin : ymax, xmin : xmax ] , detection_graph, cls = 'person')\n",
    "        \n",
    "        cnt = 0\n",
    "       \n",
    "        \n",
    "        if len(persons) is 0:\n",
    "       \n",
    "            for l in range(0,len(prev_dq)):\n",
    "                if bag in prev_dq[l]:\n",
    "                    cnt +=1\n",
    "                else:\n",
    "                    \n",
    "                    for j in range(0,len(prev_dq[l])): \n",
    "                        iou = bb_intersection_over_union(bag,prev_dq[l][j])\n",
    "                        if iou > 0.45:\n",
    "                            cnt+=1\n",
    "                            break\n",
    "            \n",
    "            if cnt <= 90:\n",
    "                cv2.rectangle(image, (bag[0], bag[1]), (bag[2], bag[3]), (255,255,0), 2)\n",
    "                cv2.putText(image,\"Unattended Lugagge \", (bag[0],bag[1]), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,0))\n",
    "            elif 90 < cnt <= 270:\n",
    "                cv2.rectangle(image, (bag[0], bag[1]), (bag[2], bag[3]), (255,165,0), 2)\n",
    "                cv2.putText(image,\"Warning Please Check\", (bag[0],bag[1]), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,165,0))\n",
    "            elif cnt > 270:\n",
    "                cv2.rectangle(image, (bag[0], bag[1]), (bag[2], bag[3]), (255,0,0), 2)\n",
    "                cv2.putText(image,\"Abandoned Luggage\", (bag[0],bag[1]), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,0,0))\n",
    "            end2 = time.time() -start2\n",
    "\n",
    "        else:\n",
    "            counter +=1\n",
    "    \n",
    "    if counter ==  len(bags):\n",
    "        cv2.putText(image,\"Normal\", (20,20), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,100,0))\n",
    "    \n",
    "    prev_dq.append(bags)\n",
    "    end_x = time.time() - start_x\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.editor import VideoFileClip\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "white_output = 'test_2_solution.mp4'\n",
    "clip1 = VideoFileClip(\"test_1.mp4\")\n",
    " \n",
    "# Process video\n",
    "white_clip = clip1.fl_image(Unattended_object_detection) #NOTE: this function expects color images!!\n",
    "get_ipython().run_line_magic('time', 'white_clip.write_videofile(white_output, audio=False)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cap = cv2.VideoCapture('test_1.mp4')\n",
    "\n",
    "# arrays = []\n",
    "\n",
    "# while True:\n",
    "    \n",
    "#     ret, frame = cap.read()\n",
    "    \n",
    "#     if ret:\n",
    "        \n",
    "#         image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "#         arrays.append(image)\n",
    "#     else:\n",
    "#         break\n",
    "# cap.release()\n",
    "# print(len(arrays))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# x = 540\n",
    "# for i in range(x, x+20):\n",
    "#     print((x+20) - i)\n",
    "#     image = Unattended_object_detection(arrays[i])\n",
    "#     plt.imshow(image)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#video clip\n",
    "\n",
    "# from moviepy.video.io.ffmpeg_tools import ffmpeg_extract_subclip\n",
    "# ffmpeg_extract_subclip(\"video1.mp4\", t1 = 30 , t2 = 70, targetname=\"test_1.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y\n",
      "0.7859007832898173\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'remove'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-9a65163e1fd0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miou\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0miou\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m0.70\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m                     \u001b[0mlist_bag\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbags\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist_bag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'remove'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "bags = np.asarray([ [121,330,165,370], [123,336,167,370]])\n",
    "list_bag = np.copy(bags)\n",
    "\n",
    "if len(list_bag)>1:\n",
    "    \n",
    "    for i in range(0, len(bags)-1):\n",
    "        \n",
    "        if bags[i] not in list_bag:\n",
    "            continue\n",
    "        else:\n",
    "            for j in range(i+1, len(bags)):\n",
    "                print('y')\n",
    "                iou = bb_intersection_over_union(bags[i],bags[j])\n",
    "                print(iou)\n",
    "                if iou>0.70:                  \n",
    "                    list_bag.remove(bags[j])\n",
    "print(list_bag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [5 6 7]]\n"
     ]
    },
    {
     "ename": "AxisError",
     "evalue": "axis 2 is out of bounds for array of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-048e797a9c8d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Conda_\\envs\\carnd-term1\\lib\\site-packages\\numpy\\lib\\function_base.py\u001b[0m in \u001b[0;36mdelete\u001b[1;34m(arr, obj, axis)\u001b[0m\n\u001b[0;32m   4785\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marrorder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4786\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4787\u001b[1;33m     \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnormalize_axis_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4788\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4789\u001b[0m     \u001b[0mslobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mslice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAxisError\u001b[0m: axis 2 is out of bounds for array of dimension 2"
     ]
    }
   ],
   "source": [
    "a = np.asarray([[1,2,3],[5,6,7]])\n",
    "print(a)\n",
    "\n",
    "b = np.delete(a,[1,2,3],axis=2)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
